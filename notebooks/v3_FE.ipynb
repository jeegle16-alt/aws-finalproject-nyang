{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49adc8bc",
   "metadata": {},
   "source": [
    "# 0. 설정 / 스키마\n",
    "## 0.1 센서/이벤트 매핑(코어/세션/이동/메타)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00670d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_USERS: 342\n",
      "SAMPLE FILES: ['u072142.csv', 'u0cba7d.csv', 'u11abb4.csv', 'u11cef8.csv', 'u11cf22.csv']\n",
      "✅ Schema constants ready (Practical & Android Aligned)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Iterable, Tuple\n",
    "\n",
    "DATA_DIR = Path(\"../data/logs\")\n",
    "CSV_PATHS = sorted(DATA_DIR.glob(\"*.csv\"))\n",
    "\n",
    "print(\"N_USERS:\", len(CSV_PATHS))\n",
    "print(\"SAMPLE FILES:\", [p.name for p in CSV_PATHS[:5]])\n",
    "\n",
    "## 0.1 센서/이벤트 매핑\n",
    "# CORE_SENSORS는 EDA에서 확인된, 실제 분석이 가능한 최소 집합입니다.\n",
    "CORE_SENSORS = [\"Screen\", \"UserAct\"]      # \"의지/행동 기반\"\n",
    "PASSIVE_SENSORS = [\"Notif\"]              # 외부자극\n",
    "AUX_SENSORS = [\"Unlock\"]                 # 세션 시작 의지(있으면 쓰고, 없으면 0)\n",
    "\n",
    "EVENT_MAP: Dict[str, str] = {\n",
    "    # 1. 핵심 행동 (UserAct는 활동의 '양'을 측정)\n",
    "    \"USER_INTERACTION\": \"UserAct\",\n",
    "    \n",
    "    # 2. 화면 상태 (Screen은 활동의 '기간'과 '세션'을 측정)\n",
    "    \"SCREEN_INTERACTIVE\": \"Screen\",\n",
    "    \"SCREEN_NON_INTERACTIVE\": \"Screen\",\n",
    "    \n",
    "    # 3. 알림 (외부 자극)\n",
    "    \"NOTIF_INTERRUPTION\": \"Notif\",\n",
    "    \n",
    "    # 4. 잠금 해제 (Unlock은 세션의 '정교한 시작점' 및 '의지'를 측정)\n",
    "    # EDA 데이터에는 없을 수 있으므로 \"보조적\"으로 활용\n",
    "    \"KEYGUARD_HIDDEN\": \"Unlock\"\n",
    "}\n",
    "\n",
    "# 공개데이터 sensor_id(대문자)도 같이 매핑되게 추가\n",
    "EVENT_MAP.update({\n",
    "    \"SCREEN\": \"Screen\",\n",
    "    \"USERACT\": \"UserAct\",\n",
    "    \"NOTIF\": \"Notif\",\n",
    "    \"UNLOCK\": \"Unlock\",        # 혹시 공개데이터에 있으면\n",
    "    \"KEYGUARD_HIDDEN\": \"Unlock\",  # 안드로이드용도 유지\n",
    "})\n",
    "\n",
    "# 분석 그룹별 대상 정의\n",
    "# Rhythm과 Gap은 '의도적 사용'을 판별하기 위해 Unlock을 포함\n",
    "RHYTHM_SENSORS = [\"Screen\", \"UserAct\", \"Unlock\"]\n",
    "GAP_SENSORS    = [\"Screen\", \"UserAct\", \"Unlock\"]\n",
    "\n",
    "# App 로그를 대신할 세션 기준: 화면 켜짐/꺼짐을 세션의 경계로 사용\n",
    "SESSION_EVENTS = [\"SCREEN_INTERACTIVE\", \"SCREEN_NON_INTERACTIVE\"]\n",
    "\n",
    "MOBILITY_EVENTS = [\"CELL_CHANGE\", \"WIFI_SSID\"]\n",
    "META_EVENT_TYPE = \"HEARTBEAT\"\n",
    "\n",
    "# Step이 없을 경우 공개데이터의 Acc_Avg를 활동량으로 대체 사용\n",
    "STEP_SENSOR_ALIASES = [\"Step_Count\", \"Step\", \"Steps\", \"STEP_COUNT\", \"STEP\", \"Acc_Avg\"]\n",
    "\n",
    "print(\"✅ Schema constants ready (Practical & Android Aligned)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8945d1dd",
   "metadata": {},
   "source": [
    "## 0.2 QC 임계값 분리(core/rhythm/gap/meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8907f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core/rhythm/gap 최소 데이터량 기준\n",
    "MIN_DAILY_EVENTS   = 50\n",
    "MIN_RHYTHM_EVENTS  = 50\n",
    "MIN_GAP_EVENTS     = 50\n",
    "\n",
    "# 의미있는 공백 기준 -> 낮잠, 외출 등\n",
    "GAP_THR_HOURS      = 2.0\n",
    "# 매우 긴 공백 기준 -> 고입, 무기력 등\n",
    "GAP_THR_6HOURS     = 6.0\n",
    "\n",
    "# meta(heartbeat) 최소 기준\n",
    "MIN_HEARTBEAT_PER_DAY = 1\n",
    "\n",
    "# 네트워크 환경이 불안정\n",
    "RETRY_WARN_THR     = 3\n",
    "QUEUE_WARN_THR     = 20\n",
    "\n",
    "# timezone 변화 감지(분) -> 해외여행\n",
    "TZ_CHANGE_THR_MINUTES = 60\n",
    "\n",
    "# \"부분수집\" 후보(코어가 너무 적은 날) -> 로그가 10개 미만\n",
    "PARTIAL_CORE_MIN_EVENTS = 10\n",
    "\n",
    "# [v3] Baseline 오염 방지용 Winsorize 범위\n",
    "WINSORIZE_LOWER = 0.005\n",
    "WINSORIZE_UPPER = 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e14b7",
   "metadata": {},
   "source": [
    "## 0.3 컬럼 표준화 헬퍼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c89c746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_COL_CANDIDATES = [\n",
    "    \"event_name\",\"sensor_id\",\"event\",\n",
    "    \"name\",\"action\",\"type_name\",\"eventCode\",\"event_id\",\n",
    "    \"event_type\"\n",
    "]\n",
    "\n",
    "TS_COL_CANDIDATES = [\n",
    "    \"ts_raw\", \"timestamp\", \"ts\",\n",
    "    \"time\", \"event_time\", \"created_at\", \"occurred_at\",\n",
    "    \"client_ts\", \"client_time\"\n",
    "]\n",
    "\n",
    "HEARTBEAT_TS_CANDIDATES = [\n",
    "    \"timestamp\", \"ts_raw\", \"ts\",\n",
    "    \"client_ts\", \"client_time\",\n",
    "    \"time\", \"event_time\", \"created_at\", \"occurred_at\"\n",
    "]\n",
    "\n",
    "BASE_USECOLS = [\n",
    "    \"uuid\",\n",
    "\n",
    "    # event 후보\n",
    "    \"sensor_id\", \"event\", \"event_name\", \"event_type\",\n",
    "    \"name\", \"action\", \"type_name\", \"eventCode\", \"event_id\",\n",
    "\n",
    "    # ts 후보\n",
    "    \"ts_raw\", \"timestamp\", \"ts\",\n",
    "    \"time\", \"event_time\", \"created_at\", \"occurred_at\",\n",
    "    \"client_ts\", \"client_time\",\n",
    "\n",
    "    # meta / heartbeat\n",
    "    \"type\", \"queue_size\", \"retry_count\", \"tz_offset_minutes\", \"client_last_event_ts\",\n",
    "\n",
    "    # mobility\n",
    "    \"cell_lac\", \"coarse_gps\", \"wifi_ssid\", \"lat\", \"lon\",\n",
    "\n",
    "    # steps\n",
    "    \"value\", \"step_count\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f896a2f",
   "metadata": {},
   "source": [
    "# 1. 데이터 로드 및 환경 설정\n",
    "## 1.1 공통 수치 유틸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac57a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활동의 '불균형도(엔트로피)'를 계산\n",
    "def entropy_from_counts(counts: np.ndarray) -> float:\n",
    "    s = float(np.nansum(counts))\n",
    "    if s <= 0:\n",
    "        return np.nan\n",
    "    p = counts / s\n",
    "    p = p[p > 0]\n",
    "    return float(-(p * np.log(p)).sum())\n",
    "\n",
    "# 여러 명의 데이터를 합칠 때 발생하는 '에러'를 방지\n",
    "def safe_concat(frames: List[pd.DataFrame], cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"빈 리스트면 빈 DF 반환. (컬럼 스키마 보장)\"\"\"\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=cols)\n",
    "    return pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22684aa",
   "metadata": {},
   "source": [
    "## 1.2 컬럼 선택/표준화 유틸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da638dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일의 헤더만 읽어옴\n",
    "def read_header_cols(path: Path) -> List[str]:\n",
    "    return pd.read_csv(path, nrows=0).columns.tolist()\n",
    "\n",
    "# 여러 후보 이름 중 이 파일에 '진짜 있는 이름'을 하나 골라냄\n",
    "def pick_first_existing_col(cols: List[str], candidates: List[str]) -> Optional[str]:\n",
    "    s = set(cols)\n",
    "    for c in candidates:\n",
    "        if c in s:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# 제각각인 컬럼명들을 '표준 이름'으로 통일\n",
    "def standardize_event_ts_cols(\n",
    "    df: pd.DataFrame,\n",
    "    event_candidates: List[str],\n",
    "    ts_candidates: List[str],\n",
    "    out_event_col: str = \"event_std\",\n",
    "    out_ts_col: str = \"ts_std\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    event 후보 컬럼 중 하나를 event_std로, ts 후보 컬럼 중 하나를 ts_std로 복사.\n",
    "    - event_std: raw 이벤트명 (string)\n",
    "    - ts_std   : raw timestamp (ms/sec/datetime/string 가능)\n",
    "    \"\"\"\n",
    "    cols = df.columns.tolist()\n",
    "    e_col = pick_first_existing_col(cols, event_candidates)\n",
    "    t_col = pick_first_existing_col(cols, ts_candidates)\n",
    "\n",
    "    if e_col is None:\n",
    "        df[out_event_col] = pd.Series([pd.NA] * len(df), dtype=\"string\")\n",
    "    else:\n",
    "        df[out_event_col] = df[e_col].astype(\"string\")\n",
    "        df[out_event_col] = df[out_event_col].where(df[out_event_col].notna(), pd.NA)\n",
    "    \n",
    "    df[out_event_col] = (\n",
    "        df[out_event_col].astype(\"string\")\n",
    "        .str.strip()\n",
    "        .str.upper()\n",
    "    )\n",
    "\n",
    "    if t_col is None:\n",
    "        df[out_ts_col] = np.nan\n",
    "    else:\n",
    "        df[out_ts_col] = df[t_col]\n",
    "\n",
    "    return df\n",
    "\n",
    "# 숫자 형태의 시간을 우리가 읽기 편한 '날짜와 시간'으로 변환\n",
    "def to_dt_date_hour_from_ms(\n",
    "    df: pd.DataFrame,\n",
    "    ts_col: str = \"ts_std\",\n",
    "    dt_col: str = \"dt\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ts(ms/sec) or datetime/string -> dt/date/hour 파생.\n",
    "    - numeric이면 자릿수 기반으로 sec vs ms 자동 판별\n",
    "    - 아니면 일반 to_datetime\n",
    "    \"\"\"\n",
    "    if ts_col not in df.columns:\n",
    "        df[dt_col] = pd.NaT\n",
    "        return df\n",
    "\n",
    "    x = df[ts_col]\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(x):\n",
    "        xn = pd.to_numeric(x, errors=\"coerce\")\n",
    "        if xn.dropna().empty:\n",
    "            df[dt_col] = pd.NaT\n",
    "            return df\n",
    "\n",
    "        med_len = xn.dropna().astype(\"int64\").astype(str).str.len().median()\n",
    "        if pd.notna(med_len) and med_len <= 10:\n",
    "            dt = pd.to_datetime(xn, unit=\"s\", errors=\"coerce\")\n",
    "        else:\n",
    "            dt = pd.to_datetime(xn, unit=\"ms\", errors=\"coerce\")\n",
    "    else:\n",
    "        dt = pd.to_datetime(x, errors=\"coerce\")\n",
    "\n",
    "    df = df.assign(**{dt_col: dt}).dropna(subset=[dt_col])\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df[\"date\"] = df[dt_col].dt.normalize()\n",
    "    df[\"hour\"] = df[dt_col].dt.hour\n",
    "    return df\n",
    "\n",
    "# 복잡한 원본 로그 이름을 우리가 정한 '쉬운 카테고리'로 바꿈 ('SCREEN_INTERACTIVE' -> 'Screen')\n",
    "def map_event_to_cat(event_std: pd.Series, event_map: Dict[str, str]) -> pd.Series:\n",
    "    e = event_std.astype(\"string\").str.strip().str.upper()\n",
    "    return e.map(event_map).astype(\"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737daa3b",
   "metadata": {},
   "source": [
    "## 1.3 chunk 로딩 (있는 컬럼만 읽기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a41033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대용량 파일을 메모리 방지를 위해 30만 줄씩 끊어서 읽기\n",
    "def read_in_chunks(\n",
    "    path: Path,\n",
    "    usecols_candidates: List[str],\n",
    "    chunksize: int = 300_000,\n",
    ") -> Iterable[pd.DataFrame]:\n",
    "    \n",
    "    cols = read_header_cols(path)\n",
    "    usecols = [c for c in usecols_candidates if c in cols]\n",
    "    if not usecols:\n",
    "        return iter(())\n",
    "    for chunk in pd.read_csv(path, usecols=usecols, chunksize=chunksize, low_memory=False):\n",
    "        if chunk is None or chunk.empty:\n",
    "            continue\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971921e",
   "metadata": {},
   "source": [
    "## 1.4 chunk 전처리: 표준화 + dt/date/hour 파생 + uuid 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6de3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이름 표준화, 시간 변환, 사용자ID 부여, 카테고리 매핑을 한 번에 수행\n",
    "def preprocess_chunk(\n",
    "    chunk: pd.DataFrame,\n",
    "    uid: str,\n",
    "    event_candidates: List[str],\n",
    "    ts_candidates: List[str],\n",
    "    event_map: Optional[Dict[str, str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    if event_map is None:\n",
    "        event_map = EVENT_MAP\n",
    "\n",
    "    if chunk is None or chunk.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 1) 표준 컬럼(event_std, ts_std)\n",
    "    chunk = standardize_event_ts_cols(\n",
    "        chunk,\n",
    "        event_candidates=event_candidates,\n",
    "        ts_candidates=ts_candidates,\n",
    "        out_event_col=\"event_std\",\n",
    "        out_ts_col=\"ts_std\",\n",
    "    )\n",
    "\n",
    "    # 2) dt/date/hour\n",
    "    chunk = to_dt_date_hour_from_ms(chunk, ts_col=\"ts_std\", dt_col=\"dt\")\n",
    "    if chunk.empty:\n",
    "        return chunk\n",
    "\n",
    "    # 3) uuid 결정 (StringDtype 유지)\n",
    "    if \"uuid\" in chunk.columns:\n",
    "        chunk[\"uuid\"] = chunk[\"uuid\"].astype(\"string\")\n",
    "        chunk[\"uuid\"] = chunk[\"uuid\"].fillna(uid)\n",
    "    else:\n",
    "        chunk[\"uuid\"] = pd.Series([uid] * len(chunk), dtype=\"string\")\n",
    "\n",
    "    # 4) ✅ event_cat 생성 (raw 이벤트 -> 카테고리)\n",
    "    #    - core/rhythm/gap는 event_cat 기준으로 갈거라서 여기서 붙여두는게 제일 안정적\n",
    "    chunk[\"event_cat\"] = map_event_to_cat(chunk[\"event_std\"], event_map)\n",
    "\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e95fe23",
   "metadata": {},
   "source": [
    "## 1.5 이벤트 필터링 helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cbe9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 이벤트명(raw)이 리스트에 포함된 행만 추출\n",
    "def filter_by_events(df: pd.DataFrame, events: List[str]) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=df.columns if df is not None else [])\n",
    "    if \"event_std\" in df.columns:\n",
    "        s = df[\"event_std\"].astype(\"string\")\n",
    "        m = s.isin(events)\n",
    "    elif \"sensor_id\" in df.columns:\n",
    "        s = df[\"sensor_id\"].astype(\"string\")\n",
    "        m = s.isin(events)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "    return df[m].copy()\n",
    "\n",
    "def filter_by_event_prefix(df: pd.DataFrame, prefixes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"event_std(raw)가 prefixes로 시작하는 행만.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=df.columns if df is not None else [])\n",
    "    s = df[\"event_std\"].astype(\"string\")\n",
    "    mask = pd.Series(False, index=df.index)\n",
    "    for p in prefixes:\n",
    "        mask |= s.str.startswith(p, na=False)\n",
    "    return df[mask].copy()\n",
    "\n",
    "# 변환된 카테고리(Screen, UserAct 등)가 리스트에 포함된 행만 추출\n",
    "def filter_by_cats(df: pd.DataFrame, cats: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"event_cat(category)가 cats에 포함되는 행만.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=df.columns if df is not None else [])\n",
    "    if \"event_cat\" not in df.columns:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "    s = df[\"event_cat\"].astype(\"string\")\n",
    "    return df[s.isin(cats)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53365cd6",
   "metadata": {},
   "source": [
    "## 1.6 하루 단위 집계 편의 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd3e6b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FE_v3. Load/Utils ready (event_cat included)\n"
     ]
    }
   ],
   "source": [
    "def ensure_date_normalized(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.normalize()\n",
    "    return df\n",
    "\n",
    "# 날짜/시간별 로그 발생 횟수를 요약표로 작성\n",
    "def group_counts(df: pd.DataFrame, keys: List[str], name: str = \"cnt\") -> pd.DataFrame:\n",
    "    \"\"\"groupby size()를 DataFrame으로 반환.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=keys + [name])\n",
    "    out = df.groupby(keys).size().reset_index(name=name)\n",
    "    return out\n",
    "\n",
    "print(\"✅ FE_v3. Load/Utils ready (event_cat included)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91797cb",
   "metadata": {},
   "source": [
    "# 2. 피처 엔지니어링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b02ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def safe_quantile(x: np.ndarray, q: float) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    if x.size == 0:\n",
    "        return np.nan\n",
    "    return float(np.quantile(x, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6779a29a",
   "metadata": {},
   "source": [
    "## 2.1 활동 기반 피처 (Activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d02363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하루 동안 발생한 핵심 센서(Screen, UserAct 등)의 횟수를 집계\n",
    "# 각 센서별로 '얼마나 많이' 움직였는지 활동의 총량을 측정\n",
    "def build_daily_activity(\n",
    "    csv_paths: List[Path],\n",
    "    core_sensors: List[str] = CORE_SENSORS,\n",
    "    chunksize: int = 300_000,\n",
    ") -> pd.DataFrame:\n",
    "    rows: List[pd.DataFrame] = []\n",
    "    usecols = BASE_USECOLS\n",
    "\n",
    "    for p in csv_paths:\n",
    "        uid = p.stem\n",
    "        # 핵심: 청크별 요약본을 담을 리스트\n",
    "        chunk_summaries: List[pd.DataFrame] = []\n",
    "\n",
    "        for chunk in read_in_chunks(p, usecols_candidates=usecols, chunksize=chunksize):\n",
    "            ch = preprocess_chunk(chunk, uid=uid, \n",
    "                                  event_candidates=EVENT_COL_CANDIDATES, \n",
    "                                  ts_candidates=TS_COL_CANDIDATES)\n",
    "            if ch.empty: continue\n",
    "\n",
    "            # 1차 필터링\n",
    "            ch = filter_by_cats(ch, core_sensors)\n",
    "            if ch.empty: continue\n",
    "\n",
    "            # [보완 1] 메모리 절약: 청크 안에서 즉시 그룹화해서 숫자로 변환\n",
    "            # 행(row)을 들고 있지 말고 '개수'만 들고 있는다.\n",
    "            summary = ch.groupby([\"uuid\", \"date\", \"event_cat\"]).size().reset_index(name=\"cnt\")\n",
    "            chunk_summaries.append(summary)\n",
    "\n",
    "        if not chunk_summaries: continue\n",
    "\n",
    "        # [보완 2] 합친 후 다시 요약 (청크 간 중복된 date/cat 합치기)\n",
    "        df_u = pd.concat(chunk_summaries, ignore_index=True)\n",
    "        daily = (\n",
    "            df_u.groupby([\"uuid\", \"date\", \"event_cat\"])[\"cnt\"]\n",
    "                .sum()\n",
    "                .unstack(fill_value=0)\n",
    "                .reset_index()\n",
    "        )\n",
    "\n",
    "        # [보완 3] 컬럼 보장 (reindex 활용으로 더 깔끔하게)\n",
    "        # core_sensors 중 로그에 없는 컬럼은 자동으로 0으로 채워짐\n",
    "        target_cols = [\"uuid\", \"date\"] + core_sensors\n",
    "        for c in core_sensors:\n",
    "            if c not in daily.columns:\n",
    "                daily[c] = 0\n",
    "        \n",
    "        rows.append(daily[target_cols])\n",
    "\n",
    "    return safe_concat(rows, [\"uuid\", \"date\"] + core_sensors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41ed30",
   "metadata": {},
   "source": [
    "## 2.2 리듬 기반 피처 (Rhythm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e81b971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_daily_rhythm(\n",
    "    csv_paths: List[Path],\n",
    "    rhythm_sensors: List[str] = RHYTHM_SENSORS,\n",
    "    chunksize: int = 300_000,\n",
    "    min_rhythm_events: int = MIN_RHYTHM_EVENTS,\n",
    ") -> pd.DataFrame:\n",
    "    out_rows: List[Dict[str, Any]] = []\n",
    "    \n",
    "    # 시간대 정의를 내부 변수로 추출 (유지보수 용이)\n",
    "    NIGHT_RANGE = (0, 6)\n",
    "    DAY_RANGE = (6, 18)\n",
    "    EVE_RANGE = (18, 24)\n",
    "\n",
    "    for p in csv_paths:\n",
    "        uid = p.stem\n",
    "        hour_counts: Dict[pd.Timestamp, np.ndarray] = {}\n",
    "\n",
    "        for chunk in read_in_chunks(p, usecols_candidates=BASE_USECOLS, chunksize=chunksize):\n",
    "            ch = preprocess_chunk(chunk, uid=uid, \n",
    "                                  event_candidates=EVENT_COL_CANDIDATES, \n",
    "                                  ts_candidates=TS_COL_CANDIDATES)\n",
    "            if ch.empty: continue\n",
    "\n",
    "            ch = filter_by_cats(ch, rhythm_sensors)\n",
    "            if ch.empty: continue\n",
    "\n",
    "            # 효율적인 누적 (이 부분은 기존 로직이 좋습니다)\n",
    "            grp = ch.groupby([\"date\", \"hour\"]).size().reset_index(name=\"cnt\")\n",
    "            for d, sub in grp.groupby(\"date\"):\n",
    "                arr = hour_counts.setdefault(d, np.zeros(24, dtype=np.int64))\n",
    "                arr[sub[\"hour\"].to_numpy()] += sub[\"cnt\"].to_numpy()\n",
    "\n",
    "        for d in sorted(hour_counts.keys()):\n",
    "            counts24 = hour_counts[d]\n",
    "            total = int(counts24.sum())\n",
    "            \n",
    "            # [보완] Coverage 체크\n",
    "            if total < min_rhythm_events:\n",
    "                out_rows.append({\n",
    "                    \"uuid\": uid, \"date\": d, \"rhythm_event_cnt\": total,\n",
    "                    \"rhythm_low_coverage\": True,\n",
    "                    **{k: np.nan for k in [\"night_ratio\", \"hour_entropy\", \n",
    "                                          \"day_ratio\", \"evening_ratio\", \"peak_hour\", \"peak_ratio\"]}\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # [보완] 피크 시간 계산 시 활동량 확인\n",
    "            max_val = np.max(counts24)\n",
    "            peak_h = int(np.argmax(counts24)) if max_val > 0 else np.nan\n",
    "            peak_r = float(max_val) / total if total > 0 else np.nan\n",
    "\n",
    "            # 엔트로피 계산\n",
    "            h_ent = entropy_from_counts(counts24.astype(float))\n",
    "            \n",
    "            # 카운트 미리 계산\n",
    "            night_cnt = int(counts24[NIGHT_RANGE[0]:NIGHT_RANGE[1]].sum())\n",
    "            day_cnt   = int(counts24[DAY_RANGE[0]:DAY_RANGE[1]].sum())\n",
    "            eve_cnt   = int(counts24[EVE_RANGE[0]:EVE_RANGE[1]].sum())\n",
    "\n",
    "            out_rows.append({\n",
    "                \"uuid\": uid, \"date\": d,\n",
    "                \"rhythm_event_cnt\": total,\n",
    "                # 1. 스무딩된 야간 비율 (주요 지표)\n",
    "                \"night_ratio\": (night_cnt + 1) / (total + 24),\n",
    "                \"hour_entropy\": h_ent if np.isfinite(h_ent) else np.nan,\n",
    "                # 2. 단순 비율들 (night_ratio는 삭제하고 day/eve만 남김)\n",
    "                \"day_ratio\": float(day_cnt / total),\n",
    "                \"evening_ratio\": float(eve_cnt / total),\n",
    "                \"peak_hour\": float(peak_h),\n",
    "                \"peak_ratio\": float(peak_r),\n",
    "                \"rhythm_low_coverage\": False,\n",
    "            })\n",
    "    if not out_rows:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"uuid\",\"date\",\n",
    "            \"rhythm_event_cnt\",\"rhythm_low_coverage\",\n",
    "            \"night_ratio\",\"hour_entropy\",\n",
    "            \"day_ratio\",\"evening_ratio\",\n",
    "            \"peak_hour\",\"peak_ratio\",\n",
    "        ])\n",
    "    return pd.DataFrame(out_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92504ca3",
   "metadata": {},
   "source": [
    "## 2.3 무활동 기반 피처 (Inactivity / Gap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4d1a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_daily_gap(\n",
    "    csv_paths: List[Path],\n",
    "    gap_sensors: List[str] = GAP_SENSORS,\n",
    "    chunksize: int = 300_000,\n",
    "    gap_thr_hours: float = GAP_THR_HOURS,\n",
    "    gap_thr_6hours: float = GAP_THR_6HOURS,\n",
    "    min_gap_events: int = MIN_GAP_EVENTS,\n",
    "    include_overnight: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    out_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for p in csv_paths:\n",
    "        uid = p.stem\n",
    "        gaps_by_date = {}\n",
    "        event_cnt_by_date = {}\n",
    "        first_dt_by_date = {}\n",
    "        last_dt_by_date = {}\n",
    "        # [수정] 청크 간 연속성을 위해 날짜별 마지막 타임스탬프를 엄격히 관리\n",
    "        last_ts_per_date = {} \n",
    "\n",
    "        for chunk in read_in_chunks(p, usecols_candidates=BASE_USECOLS, chunksize=chunksize):\n",
    "            ch = preprocess_chunk(chunk, uid=uid, \n",
    "                                  event_candidates=EVENT_COL_CANDIDATES, \n",
    "                                  ts_candidates=TS_COL_CANDIDATES)\n",
    "            if ch.empty: continue\n",
    "            ch = filter_by_cats(ch, gap_sensors)\n",
    "            if ch.empty: continue\n",
    "\n",
    "            # 청크 내 시간 정렬\n",
    "            ch = ch.sort_values(\"dt\")\n",
    "            \n",
    "            for d, sub in ch.groupby(\"date\"):\n",
    "                d_idx = pd.Timestamp(d)\n",
    "                event_cnt_by_date[d_idx] = event_cnt_by_date.get(d_idx, 0) + len(sub)\n",
    "                \n",
    "                s_first, s_last = sub[\"dt\"].iloc[0], sub[\"dt\"].iloc[-1]\n",
    "                \n",
    "                # 전역 처음/마지막 업데이트\n",
    "                first_dt_by_date[d_idx] = min(first_dt_by_date.get(d_idx, s_first), s_first)\n",
    "                last_dt_by_date[d_idx]  = max(last_dt_by_date.get(d_idx, s_last), s_last)\n",
    "\n",
    "                # [보완] 청크 사이의 Gap 계산 (이전 청크의 마지막 시간 기준)\n",
    "                if d_idx in last_ts_per_date:\n",
    "                    cross = (s_first - last_ts_per_date[d_idx]).total_seconds() / 3600.0\n",
    "                    if cross > 0:\n",
    "                        gaps_by_date.setdefault(d_idx, []).append(cross)\n",
    "\n",
    "                # 청크 내부 Gap 계산\n",
    "                if len(sub) > 1:\n",
    "                    diffs = sub[\"dt\"].diff().dt.total_seconds().dropna().to_numpy() / 3600.0\n",
    "                    gaps_by_date.setdefault(d_idx, []).extend(diffs[diffs > 0].tolist())\n",
    "\n",
    "                # 다음 청크를 위해 현재 청크의 마지막 시간 저장\n",
    "                last_ts_per_date[d_idx] = s_last\n",
    "\n",
    "        # Overnight Gap (전일 -> 당일)\n",
    "        overnight_gap_by_date = {}\n",
    "        if include_overnight:\n",
    "            sorted_days = sorted(first_dt_by_date.keys())\n",
    "            for i in range(1, len(sorted_days)):\n",
    "                prev_d, curr_d = sorted_days[i-1], sorted_days[i]\n",
    "                if (curr_d - prev_d).days == 1:\n",
    "                    og = (first_dt_by_date[curr_d] - last_dt_by_date[prev_d]).total_seconds() / 3600.0\n",
    "                    if og >= 0: overnight_gap_by_date[curr_d] = og\n",
    "\n",
    "        # 최종 집계\n",
    "        for d in sorted(event_cnt_by_date.keys()):\n",
    "            total = event_cnt_by_date[d]\n",
    "            og_val = overnight_gap_by_date.get(d, np.nan)\n",
    "            \n",
    "            # 기본값 셋업\n",
    "            res = {\n",
    "                \"uuid\": uid, \"date\": d, \"gap_event_cnt\": total,\n",
    "                \"overnight_gap\": og_val,\n",
    "                \"first_hour\": float(first_dt_by_date[d].hour),\n",
    "                \"last_hour\": float(last_dt_by_date[d].hour),\n",
    "                \"gap_low_coverage\": False\n",
    "            }\n",
    "\n",
    "            if total < min_gap_events:\n",
    "                res.update({\"gap_low_coverage\": True, \"gap_max\": np.nan, \"gap_p95\": np.nan, \n",
    "                            \"gap_cnt_2h\": np.nan, \"gap_cnt_6h\": np.nan, \"gap_long_ratio\": np.nan})\n",
    "            else:\n",
    "                combined_gaps = gaps_by_date.get(d, []).copy()\n",
    "                if not np.isnan(og_val): combined_gaps.append(og_val)\n",
    "                gaps = np.array(combined_gaps, dtype=float)\n",
    "\n",
    "                if gaps.size == 0:\n",
    "                    stats = {\"gap_max\": np.nan, \"gap_p95\": np.nan, \"gap_cnt_2h\": 0, \"gap_cnt_6h\": 0, \"gap_long_ratio\": 0.0}\n",
    "                else:\n",
    "                    l_sum = float(np.sum(gaps[gaps >= gap_thr_hours]))\n",
    "                    stats = {\n",
    "                        \"gap_max\": float(np.max(gaps)),\n",
    "                        \"gap_p95\": float(safe_quantile(gaps, 0.95)),\n",
    "                        \"gap_cnt_2h\": int(np.sum(gaps >= gap_thr_hours)),\n",
    "                        \"gap_cnt_6h\": int(np.sum(gaps >= gap_thr_6hours)),\n",
    "                        \"gap_long_ratio\": min(l_sum / 24.0, 1.0)\n",
    "                    }\n",
    "                res.update(stats)\n",
    "            \n",
    "            out_rows.append(res)\n",
    "    if not out_rows:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"uuid\",\"date\",\"gap_event_cnt\",\"gap_low_coverage\",\n",
    "            \"gap_max\",\"gap_p95\",\"gap_cnt_2h\",\"gap_cnt_6h\",\n",
    "            \"gap_long_ratio\",\"overnight_gap\",\"first_hour\",\"last_hour\"\n",
    "        ])\n",
    "    return pd.DataFrame(out_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb95df3",
   "metadata": {},
   "source": [
    "## 2.4 세션 기반 피처(Session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49e0b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앱 사용의 시작(RESUMED)과 끝(PAUSED) 사이의 지속 시간 계산\n",
    "# 한 번 폰을 잡으면 얼마나 오래 쓰는지, 30분 이상의 '긴 세션'은 몇 번인지 집계\n",
    "def build_daily_session(\n",
    "    csv_paths: List[Path],\n",
    "    session_events: List[str] = SESSION_EVENTS,\n",
    "    chunksize: int = 300_000,\n",
    "    long_session_sec: int = 30 * 60,\n",
    ") -> pd.DataFrame:\n",
    "    out_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    START = \"SCREEN_INTERACTIVE\"\n",
    "    END   = \"SCREEN_NON_INTERACTIVE\"\n",
    "\n",
    "    for p in csv_paths:\n",
    "        uid = p.stem\n",
    "        seq_by_date: Dict[pd.Timestamp, List[Tuple[pd.Timestamp, str]]] = {}\n",
    "\n",
    "        for chunk in read_in_chunks(p, usecols_candidates=BASE_USECOLS, chunksize=chunksize):\n",
    "            ch = preprocess_chunk(chunk, uid=uid, \n",
    "                                  event_candidates=EVENT_COL_CANDIDATES, \n",
    "                                  ts_candidates=TS_COL_CANDIDATES)\n",
    "            if ch.empty: continue\n",
    "            ch = filter_by_events(ch, session_events)\n",
    "            if ch.empty: continue\n",
    "\n",
    "            # 정렬 및 데이터 수집\n",
    "            ch = ch.sort_values([\"date\", \"dt\"])\n",
    "            for d, sub in ch.groupby(\"date\"):\n",
    "                d_idx = pd.Timestamp(d)\n",
    "                seq_by_date.setdefault(d_idx, []).extend(list(zip(sub[\"dt\"], sub[\"event_std\"])))\n",
    "\n",
    "        for d in sorted(seq_by_date.keys()):\n",
    "            # [보완] 시계열 순서 보장\n",
    "            seq = sorted(seq_by_date[d], key=lambda x: x[0])\n",
    "            \n",
    "            durations: List[float] = []\n",
    "            open_start: Optional[pd.Timestamp] = None\n",
    "            \n",
    "            for t, ev in seq:\n",
    "                if ev == START:\n",
    "                    # 이미 열려있다면 무시 (중복 방어)\n",
    "                    if open_start is None:\n",
    "                        open_start = t\n",
    "                elif ev == END:\n",
    "                    if open_start is not None:\n",
    "                        dur = (t - open_start).total_seconds()\n",
    "                        # [보완] 0초보다 크고, 사람이 현실적으로 할 수 없는 긴 세션(예: 6시간) 방어\n",
    "                        if 0 < dur < (6 * 3600): \n",
    "                            durations.append(dur)\n",
    "                        open_start = None\n",
    "\n",
    "            # 집계 로직\n",
    "            session_cnt = len(durations)\n",
    "            if session_cnt == 0:\n",
    "                out_rows.append({\n",
    "                    \"uuid\": uid, \"date\": d,\n",
    "                    \"session_cnt\": 0, \"session_total_sec\": 0.0,\n",
    "                    \"session_mean_sec\": np.nan, \"long_session_cnt\": 0,\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            dur_arr = np.array(durations)\n",
    "            out_rows.append({\n",
    "                \"uuid\": uid, \"date\": d,\n",
    "                \"session_cnt\": int(session_cnt),\n",
    "                \"session_total_sec\": float(np.sum(dur_arr)),\n",
    "                \"session_mean_sec\": float(np.mean(dur_arr)),\n",
    "                \"long_session_cnt\": int(np.sum(dur_arr >= long_session_sec)),\n",
    "            })\n",
    "    if not out_rows:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"uuid\",\"date\",\"session_cnt\",\"session_total_sec\",\"session_mean_sec\",\"long_session_cnt\"\n",
    "        ])\n",
    "    return pd.DataFrame(out_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b4a312",
   "metadata": {},
   "source": [
    "## 2.5 이동성 기반 피처(Mobility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c2de17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_daily_mobility(\n",
    "    csv_paths: List[Path],\n",
    "    mobility_events: List[str] = MOBILITY_EVENTS, \n",
    "    step_aliases: List[str] = STEP_SENSOR_ALIASES,\n",
    "    chunksize: int = 300_000,\n",
    ") -> pd.DataFrame:\n",
    "    out_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for p in csv_paths:\n",
    "        uid = p.stem\n",
    "        cell_cnt, cell_uniqs, step_sum = {}, {}, {}\n",
    "        unique_wifi_ssids = {}\n",
    "        wifi_change_cnt_est = {}\n",
    "        \n",
    "        # [추가] 연속 SSID 변화를 추적하기 위한 청크 간 상태 저장\n",
    "        last_ssid_seen = {} \n",
    "\n",
    "        for chunk in read_in_chunks(p, usecols_candidates=BASE_USECOLS, chunksize=chunksize):\n",
    "            ch = preprocess_chunk(chunk, uid=uid, \n",
    "                                  event_candidates=EVENT_COL_CANDIDATES, \n",
    "                                  ts_candidates=TS_COL_CANDIDATES)\n",
    "            if ch.empty: continue\n",
    "\n",
    "            # --- 1) CELL / WIFI 기반 이동성 분석 ---\n",
    "            ch_m = filter_by_events(ch, mobility_events).sort_values(\"dt\")\n",
    "            if not ch_m.empty:\n",
    "                for d, sub in ch_m.groupby(\"date\"):\n",
    "                    d_idx = pd.Timestamp(d)\n",
    "                    \n",
    "                    # (1) cell_change_cnt: CELL_CHANGE 이벤트 단순 합산\n",
    "                    c_cnt = len(sub[sub[\"event_std\"] == \"CELL_CHANGE\"])\n",
    "                    cell_cnt[d_idx] = cell_cnt.get(d_idx, 0) + c_cnt\n",
    "                    \n",
    "                    # (2) unique_cell_cnt: cell_lac의 고유 집합 크기\n",
    "                    if \"cell_lac\" in sub.columns:\n",
    "                        cells = sub[sub[\"event_std\"] == \"CELL_CHANGE\"][\"cell_lac\"].dropna().astype(str)\n",
    "                        cell_uniqs.setdefault(d_idx, set()).update(cells.tolist())\n",
    "\n",
    "                    # (3) wifi_change_cnt_est & unique_wifi_cnt\n",
    "                    if \"wifi_ssid\" in sub.columns:\n",
    "                        wifi_sub = sub[sub[\"event_std\"] == \"WIFI_SSID\"].copy()\n",
    "                        wifi_sub[\"wifi_ssid\"] = wifi_sub[\"wifi_ssid\"].astype(str)\n",
    "                        \n",
    "                        if not wifi_sub.empty:\n",
    "                            # unique_wifi_cnt용 set 업데이트\n",
    "                            unique_wifi_ssids.setdefault(d_idx, set()).update(wifi_sub[\"wifi_ssid\"].tolist())\n",
    "                            \n",
    "                            # 연속 SSID 변화 카운트 (환경 변화)\n",
    "                            ssids = wifi_sub[\"wifi_ssid\"].values\n",
    "                            # 이전 청크의 마지막 SSID와 현재 청크의 첫 SSID 비교\n",
    "                            current_last = last_ssid_seen.get(d_idx)\n",
    "                            \n",
    "                            changes = 0\n",
    "                            for i in range(len(ssids)):\n",
    "                                if current_last is not None and ssids[i] != current_last:\n",
    "                                    changes += 1\n",
    "                                current_last = ssids[i]\n",
    "                            \n",
    "                            wifi_change_cnt_est[d_idx] = wifi_change_cnt_est.get(d_idx, 0) + changes\n",
    "                            last_ssid_seen[d_idx] = current_last\n",
    "\n",
    "            # --- 2) Step Count 기반 활동량 분석 ---\n",
    "            is_step = ch[\"event_std\"].isin(step_aliases)\n",
    "            if \"sensor_id\" in ch.columns:\n",
    "                is_step |= ch[\"sensor_id\"].astype(str).isin(step_aliases)\n",
    "            \n",
    "            ch_s = ch[is_step].copy()\n",
    "            if not ch_s.empty:\n",
    "                val_col = next((c for c in [\"step_count\", \"value\"] if c in ch_s.columns), None)\n",
    "                if val_col:\n",
    "                    ch_s[val_col] = pd.to_numeric(ch_s[val_col], errors=\"coerce\")\n",
    "                    # 중복 로그 방어\n",
    "                    ch_s = ch_s.loc[~ch_s.index.duplicated(keep='first')]\n",
    "                    for d, s in ch_s.groupby(\"date\")[val_col].sum().items():\n",
    "                        d_idx = pd.Timestamp(d)\n",
    "                        step_sum[d_idx] = step_sum.get(d_idx, 0.0) + float(s)\n",
    "\n",
    "        # 최종 결과 구성\n",
    "        all_dates = sorted(set(cell_cnt) | set(step_sum) | set(unique_wifi_ssids))\n",
    "        for d in all_dates:\n",
    "            out_rows.append({\n",
    "                \"uuid\": uid,\n",
    "                \"date\": d,\n",
    "                \"cell_change_cnt\": int(cell_cnt.get(d, 0)),             # 이동량\n",
    "                \"wifi_change_cnt_est\": int(wifi_change_cnt_est.get(d, 0)), # 환경 변화\n",
    "                \"unique_wifi_cnt\": float(len(unique_wifi_ssids.get(d, set()))), # 장소 다양성\n",
    "                \"unique_cell_cnt\": float(len(cell_uniqs.get(d, set()))),       # 광역 장소 수\n",
    "                \"step_sum\": step_sum.get(d, np.nan),                    # 활동량\n",
    "            })\n",
    "    if not out_rows:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"uuid\",\"date\",\"cell_change_cnt\",\"wifi_change_cnt_est\",\"unique_wifi_cnt\",\"unique_cell_cnt\",\"step_sum\"\n",
    "        ])\n",
    "    return pd.DataFrame(out_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef397661",
   "metadata": {},
   "source": [
    "##  2.6 운영 메타/QC 피처(Meta/QC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e189cd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ build_daily_meta_qc patched (heartbeat ts candidates + dual heartbeat detection)\n"
     ]
    }
   ],
   "source": [
    "# 하트비트 신호와 시스템 상태(배터리, 큐 사이즈, 리트라이) 확인\n",
    "# 데이터가 누락 없이 잘 전송되었는지, 타임존이 바뀌었는지(해외여행 등) 품질을 체크\n",
    "def build_daily_meta_qc(\n",
    "    csv_paths: List[Path],\n",
    "    meta_event_type: str = META_EVENT_TYPE,   # \"heartbeat\"\n",
    "    chunksize: int = 300_000,\n",
    ") -> pd.DataFrame:\n",
    "    out_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for p in csv_paths:\n",
    "        uid = p.stem\n",
    "        # 집계용 딕셔너리\n",
    "        hb_cnt, retry_max, queue_max = {}, {}, {}\n",
    "        tz_min, tz_max, last_event_max = {}, {}, {}\n",
    "\n",
    "        for chunk in read_in_chunks(p, usecols_candidates=BASE_USECOLS, chunksize=chunksize):\n",
    "            if chunk is None or chunk.empty: continue\n",
    "            ch = chunk.copy()\n",
    "\n",
    "            # (1) Heartbeat 행 필터링\n",
    "            ch_hb = pd.DataFrame()\n",
    "            if \"type\" in ch.columns:\n",
    "                m = ch[\"type\"].astype(\"string\").str.lower() == str(meta_event_type).lower()\n",
    "                ch_hb = ch[m].copy()\n",
    "            \n",
    "            if ch_hb.empty:\n",
    "                ch2 = standardize_event_ts_cols(ch, \n",
    "                                                event_candidates=EVENT_COL_CANDIDATES,\n",
    "                                                ts_candidates=HEARTBEAT_TS_CANDIDATES)\n",
    "                m = ch2[\"event_std\"].astype(\"string\").str.strip().str.lower() == str(meta_event_type).lower()\n",
    "                ch_hb = ch2[m].copy()\n",
    "\n",
    "            if ch_hb.empty: continue\n",
    "\n",
    "            # (2) 시간 변환 및 날짜 추출\n",
    "            ts_col = pick_first_existing_col(ch_hb.columns.tolist(), HEARTBEAT_TS_CANDIDATES)\n",
    "            if ts_col is None: continue\n",
    "\n",
    "            x = pd.to_numeric(ch_hb[ts_col], errors=\"coerce\")\n",
    "            if x.dropna().empty: continue\n",
    "            \n",
    "            # 단위(s/ms) 판별 및 변환\n",
    "            med_len = x.dropna().astype(\"int64\").astype(str).str.len().median()\n",
    "            unit = \"s\" if (pd.notna(med_len) and med_len <= 10) else \"ms\"\n",
    "            ch_hb[\"dt_hb\"] = pd.to_datetime(x, unit=unit, errors=\"coerce\")\n",
    "            ch_hb = ch_hb.dropna(subset=[\"dt_hb\"])\n",
    "            if ch_hb.empty: continue\n",
    "            ch_hb[\"date\"] = ch_hb[\"dt_hb\"].dt.normalize()\n",
    "\n",
    "            # (3) 집계 로직\n",
    "            for d, sub in ch_hb.groupby(\"date\"):\n",
    "                d_idx = pd.Timestamp(d)\n",
    "                hb_cnt[d_idx] = hb_cnt.get(d_idx, 0) + len(sub)\n",
    "\n",
    "                # 수치 데이터 업데이트 헬퍼\n",
    "                def update_target(col_name, target_dict, kind: str):\n",
    "                    if col_name not in sub.columns:\n",
    "                        return\n",
    "                    vals = pd.to_numeric(sub[col_name], errors=\"coerce\").dropna()\n",
    "                    if vals.empty:\n",
    "                        return\n",
    "\n",
    "                    new_val = float(vals.max() if kind == \"max\" else vals.min())\n",
    "                    prev_val = target_dict.get(d_idx, np.nan)\n",
    "\n",
    "                    if not np.isfinite(prev_val):\n",
    "                        target_dict[d_idx] = new_val\n",
    "                    else:\n",
    "                        target_dict[d_idx] = max(prev_val, new_val) if kind == \"max\" else min(prev_val, new_val)\n",
    "\n",
    "                update_target(\"retry_count\", retry_max, \"max\")\n",
    "                update_target(\"queue_size\", queue_max, \"max\")\n",
    "                update_target(\"tz_offset_minutes\", tz_min, \"min\")\n",
    "                update_target(\"tz_offset_minutes\", tz_max, \"max\")\n",
    "                update_target(\"client_last_event_ts\", last_event_max, \"max\")\n",
    "\n",
    "        # (4) 최종 데이터 구성\n",
    "        for d in sorted(hb_cnt.keys()):\n",
    "            tmin = tz_min.get(d, np.nan)\n",
    "            tmax = tz_max.get(d, np.nan)\n",
    "            \n",
    "            # 타임존 변경 감지 (기본 임계값 30분~60분)\n",
    "            tz_changed = False\n",
    "            if pd.notna(tmin) and pd.notna(tmax):\n",
    "                tz_changed = bool(abs(tmax - tmin) >= TZ_CHANGE_THR_MINUTES)\n",
    "\n",
    "            out_rows.append({\n",
    "                \"uuid\": uid, \"date\": d,\n",
    "                \"heartbeat_cnt\": int(hb_cnt[d]),\n",
    "                \"retry_max\": retry_max.get(d, np.nan),\n",
    "                \"queue_max\": queue_max.get(d, np.nan),\n",
    "                \"tz_offset_min\": tmin,\n",
    "                \"tz_offset_max\": tmax,\n",
    "                \"tz_changed\": tz_changed,\n",
    "                \"qc_last_ts_max\": last_event_max.get(d, np.nan)\n",
    "            })\n",
    "    if not out_rows:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"uuid\",\"date\",\"heartbeat_cnt\",\"retry_max\",\"queue_max\",\n",
    "            \"tz_offset_min\",\"tz_offset_max\",\"tz_changed\",\"qc_last_ts_max\"\n",
    "        ])\n",
    "    return pd.DataFrame(out_rows)\n",
    "print(\"✅ build_daily_meta_qc patched (heartbeat ts candidates + dual heartbeat detection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50beae28",
   "metadata": {},
   "source": [
    "# 3. 병합 + 파생 피처\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef1b9c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _assert_or_dedup_uuid_date(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"(uuid,date) 중복이 있으면 폭발 방지용으로 강제 정리.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    if not {\"uuid\", \"date\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"{name}: missing uuid/date columns\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.normalize()\n",
    "\n",
    "    dup = int(df.duplicated([\"uuid\", \"date\"]).sum())\n",
    "    if dup == 0:\n",
    "        return df\n",
    "\n",
    "    out = (\n",
    "        df.sort_values([\"uuid\", \"date\"])\n",
    "        .drop_duplicates([\"uuid\", \"date\"], keep=\"last\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    print(f\"⚠️ [{name}] duplicated (uuid,date)={dup} -> keep last row\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def _outer_merge_on_uuid_date(left: pd.DataFrame, right: pd.DataFrame) -> pd.DataFrame:\n",
    "    if left is None or left.empty:\n",
    "        return right.copy()\n",
    "    if right is None or right.empty:\n",
    "        return left.copy()\n",
    "    return left.merge(right, on=[\"uuid\", \"date\"], how=\"outer\")\n",
    "\n",
    "\n",
    "def _to01_or_nan(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"bool/float/nan 섞여도 0/1/NaN 형태로 정리.\"\"\"\n",
    "    if s is None:\n",
    "        return s\n",
    "    x = s.copy()\n",
    "    x = x.where(~pd.isna(x), np.nan)\n",
    "    if x.dtype == bool:\n",
    "        return x.astype(float)\n",
    "    if pd.api.types.is_object_dtype(x):\n",
    "        x = x.replace({\"True\": 1, \"False\": 0, True: 1, False: 0})\n",
    "    return pd.to_numeric(x, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49556228",
   "metadata": {},
   "source": [
    "## 3.1 outer merge (uuid,date union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f9b77db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_daily_feature_table_v3(csv_paths: List[Path]) -> pd.DataFrame:\n",
    "    # 2.x 블록별 피처 생성\n",
    "    activity_sensors = CORE_SENSORS + PASSIVE_SENSORS + AUX_SENSORS\n",
    "    activity_df = build_daily_activity(csv_paths, core_sensors=activity_sensors)  \n",
    "    rhythm_df   = build_daily_rhythm(csv_paths)\n",
    "    gap_df      = build_daily_gap(csv_paths)\n",
    "    session_df  = build_daily_session(csv_paths)\n",
    "    mob_df      = build_daily_mobility(csv_paths)\n",
    "    meta_df     = build_daily_meta_qc(csv_paths)\n",
    "\n",
    "    # ✅ merge 폭발 방지: (uuid,date) 유니크 보장\n",
    "    activity_df = _assert_or_dedup_uuid_date(activity_df, \"activity\")\n",
    "    rhythm_df   = _assert_or_dedup_uuid_date(rhythm_df,   \"rhythm\")\n",
    "    gap_df      = _assert_or_dedup_uuid_date(gap_df,      \"gap\")\n",
    "    session_df  = _assert_or_dedup_uuid_date(session_df,  \"session\")\n",
    "    mob_df      = _assert_or_dedup_uuid_date(mob_df,      \"mobility\")\n",
    "    meta_df     = _assert_or_dedup_uuid_date(meta_df,     \"meta\")\n",
    "\n",
    "    # outer merge (uuid,date union)\n",
    "    df = activity_df.copy()\n",
    "    df = _outer_merge_on_uuid_date(df, rhythm_df)\n",
    "    df = _outer_merge_on_uuid_date(df, gap_df)\n",
    "    df = _outer_merge_on_uuid_date(df, session_df)\n",
    "    df = _outer_merge_on_uuid_date(df, mob_df)\n",
    "    df = _outer_merge_on_uuid_date(df, meta_df)\n",
    "\n",
    "    # date normalize + sort\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.normalize()\n",
    "    df = df.dropna(subset=[\"uuid\", \"date\"]).sort_values([\"uuid\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    # Unlock count 표준화 (없으면 0)\n",
    "    if \"Unlock\" not in df.columns:\n",
    "        df[\"Unlock\"] = 0\n",
    "    df[\"Unlock\"] = pd.to_numeric(df[\"Unlock\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    df[\"unlock_cnt\"] = df[\"Unlock\"]\n",
    "\n",
    "    for c in CORE_SENSORS:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "    # PASSIVE도 dtype/결측 정리\n",
    "    for c in PASSIVE_SENSORS:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # unlock_cnt도 dtype 확정\n",
    "    if \"unlock_cnt\" in df.columns:\n",
    "        df[\"unlock_cnt\"] = pd.to_numeric(df[\"unlock_cnt\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # daily event (core 합)\n",
    "    core_cols_present = [c for c in CORE_SENSORS if c in df.columns]\n",
    "    df[\"daily_event_cnt\"] = df[core_cols_present].sum(axis=1).astype(int)\n",
    "    df[\"has_activity\"] = (df[\"daily_event_cnt\"] > 0)\n",
    "\n",
    "    # 3.2 QC flags\n",
    "    df = add_qc_flags_v3(df)\n",
    "\n",
    "    # 3.3 soft context\n",
    "    df = add_soft_context_signals_v3(df)\n",
    "\n",
    "    # 3.4 delta features\n",
    "    df = add_delta_features_v3(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd9bbf",
   "metadata": {},
   "source": [
    "## 3.2 QC flags 분리(core/rhythm/gap/meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cab535f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_qc_flags_v3(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # (A) core QC\n",
    "    df[\"qc_core_low_cov\"] = df[\"daily_event_cnt\"] < MIN_DAILY_EVENTS\n",
    "    df[\"qc_core_very_low_activity\"] = (df[\"daily_event_cnt\"] > 0) & (df[\"daily_event_cnt\"] < PARTIAL_CORE_MIN_EVENTS)\n",
    "\n",
    "    # (B) rhythm QC\n",
    "    if \"rhythm_low_coverage\" not in df.columns:\n",
    "        df[\"rhythm_low_coverage\"] = True\n",
    "    df[\"rhythm_low_coverage\"] = df[\"rhythm_low_coverage\"].fillna(True).astype(bool)\n",
    "    df[\"qc_rhythm_low_cov\"] = df[\"rhythm_low_coverage\"]\n",
    "\n",
    "    # (C) gap QC\n",
    "    if \"gap_low_coverage\" not in df.columns:\n",
    "        df[\"gap_low_coverage\"] = True\n",
    "    df[\"gap_low_coverage\"] = df[\"gap_low_coverage\"].fillna(True).astype(bool)\n",
    "    df[\"qc_gap_low_cov\"] = df[\"gap_low_coverage\"]\n",
    "\n",
    "    # (D) meta QC\n",
    "    if \"heartbeat_cnt\" in df.columns:\n",
    "        df[\"qc_meta_low_heartbeat\"] = (\n",
    "            pd.to_numeric(df[\"heartbeat_cnt\"], errors=\"coerce\").fillna(0).astype(int) < MIN_HEARTBEAT_PER_DAY\n",
    "        )\n",
    "    else:\n",
    "        df[\"qc_meta_low_heartbeat\"] = np.nan\n",
    "\n",
    "    if \"retry_max\" in df.columns:\n",
    "        df[\"qc_meta_retry_warn\"] = pd.to_numeric(df[\"retry_max\"], errors=\"coerce\").fillna(-np.inf) >= RETRY_WARN_THR\n",
    "    else:\n",
    "        df[\"qc_meta_retry_warn\"] = np.nan\n",
    "\n",
    "    if \"queue_max\" in df.columns:\n",
    "        df[\"qc_meta_queue_warn\"] = pd.to_numeric(df[\"queue_max\"], errors=\"coerce\").fillna(-np.inf) >= QUEUE_WARN_THR\n",
    "    else:\n",
    "        df[\"qc_meta_queue_warn\"] = np.nan\n",
    "\n",
    "    # tz_changed fallback\n",
    "    if \"tz_changed\" not in df.columns and (\"tz_offset_min\" in df.columns) and (\"tz_offset_max\" in df.columns):\n",
    "        tmin = pd.to_numeric(df[\"tz_offset_min\"], errors=\"coerce\")\n",
    "        tmax = pd.to_numeric(df[\"tz_offset_max\"], errors=\"coerce\")\n",
    "        df[\"tz_changed\"] = (tmax - tmin).abs() >= TZ_CHANGE_THR_MINUTES\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7494e15",
   "metadata": {},
   "source": [
    "## 3.3 soft context signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "898c51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_soft_context_signals_v3(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # (1) tz 변화 신호\n",
    "    if \"tz_changed\" in df.columns:\n",
    "        df[\"tz_change_signal\"] = _to01_or_nan(df[\"tz_changed\"].astype(\"boolean\"))\n",
    "    else:\n",
    "        df[\"tz_change_signal\"] = 0.0\n",
    "\n",
    "    # (2) partial 신호\n",
    "    core_very_low = df.get(\"qc_core_very_low_activity\", False).fillna(False).astype(bool)\n",
    "    retry_warn = df.get(\"qc_meta_retry_warn\", np.nan)\n",
    "    queue_warn = df.get(\"qc_meta_queue_warn\", np.nan)\n",
    "\n",
    "    retry01 = _to01_or_nan(retry_warn)\n",
    "    queue01 = _to01_or_nan(queue_warn)\n",
    "\n",
    "    base = core_very_low.astype(int)\n",
    "    df[\"partial_signal_raw\"] = (\n",
    "        base\n",
    "        + pd.to_numeric(retry01, errors=\"coerce\").fillna(0).astype(int)\n",
    "        + pd.to_numeric(queue01, errors=\"coerce\").fillna(0).astype(int)\n",
    "    ).astype(float)\n",
    "    df.loc[~core_very_low, \"partial_signal_raw\"] = 0.0\n",
    "\n",
    "    # (3) travel 신호\n",
    "    travel = pd.Series(0, index=df.index, dtype=float)\n",
    "\n",
    "    tz01 = pd.to_numeric(df[\"tz_change_signal\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    travel += (tz01 * 2).astype(float)\n",
    "\n",
    "    if \"cell_change_cnt\" in df.columns:\n",
    "        c = pd.to_numeric(df[\"cell_change_cnt\"], errors=\"coerce\")\n",
    "        c_prev = c.groupby(df[\"uuid\"]).shift(1)\n",
    "        travel += ((c_prev.notna()) & (c >= (c_prev * 2 + 10))).astype(float)\n",
    "\n",
    "    if \"wifi_change_cnt_est\" in df.columns:\n",
    "        w = pd.to_numeric(df[\"wifi_change_cnt_est\"], errors=\"coerce\")\n",
    "        w_prev = w.groupby(df[\"uuid\"]).shift(1)\n",
    "        travel += ((w_prev.notna()) & (w >= (w_prev * 2 + 10))).astype(float)\n",
    "\n",
    "    if \"unique_cell_cnt\" in df.columns:\n",
    "        u1 = pd.to_numeric(df.get(\"unique_cell_cnt\", np.nan), errors=\"coerce\").fillna(0)\n",
    "        u1_prev = u1.groupby(df[\"uuid\"]).shift(1)\n",
    "        travel += ((u1_prev.notna()) & (u1 >= (u1_prev + 3))).astype(float)\n",
    "\n",
    "    df[\"travel_signal_raw\"] = travel\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf5a6b",
   "metadata": {},
   "source": [
    " ## 3.4 전일 대비 변화량(delta features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d132a768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FE_v3 merge/derive ready\n"
     ]
    }
   ],
   "source": [
    "def add_delta_features_v3(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df = df.sort_values([\"uuid\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    delta_cols = [\n",
    "        \"daily_event_cnt\",\n",
    "        \"night_ratio\",\n",
    "        \"hour_entropy\",\n",
    "        \"gap_max\",\n",
    "        \"gap_p95\",\n",
    "        \"gap_cnt_2h\",\n",
    "        \"gap_cnt_6h\",\n",
    "        \"session_total_sec\",\n",
    "        \"session_cnt\",\n",
    "        \"step_sum\",\n",
    "        \"cell_change_cnt\",\n",
    "        \"wifi_change_cnt_est\",\n",
    "        \"travel_signal_raw\",\n",
    "        \"partial_signal_raw\",\n",
    "        \"tz_change_signal\",\n",
    "    ]\n",
    "    delta_cols = [c for c in delta_cols if c in df.columns]\n",
    "\n",
    "    for c in delta_cols:\n",
    "        x = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        prev = x.groupby(df[\"uuid\"]).shift(1)\n",
    "        df[f\"{c}_d1\"] = x - prev\n",
    "\n",
    "    ratio_cols = [\"daily_event_cnt\", \"session_total_sec\", \"step_sum\"]\n",
    "    ratio_cols = [c for c in ratio_cols if c in df.columns]\n",
    "\n",
    "    eps = 1e-6\n",
    "    for c in ratio_cols:\n",
    "        x = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        prev = x.groupby(df[\"uuid\"]).shift(1)\n",
    "        df[f\"{c}_r1\"] = (x - prev) / (prev.abs() + eps)\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"✅ FE_v3 merge/derive ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6b707d",
   "metadata": {},
   "source": [
    "# 4. 저장용 컬럼 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42828b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_daily_feature_v3(df: pd.DataFrame, out_dir: Path) -> Path:\n",
    "    df = df.copy()\n",
    "\n",
    "    # meta 컬럼 없으면 NaN 채워서 스키마 고정\n",
    "    for c in [\n",
    "        \"tz_changed\",\"tz_offset_min\",\"tz_offset_max\",\n",
    "        \"heartbeat_cnt\",\"retry_max\",\"queue_max\",\"qc_last_ts_max\"\n",
    "    ]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cols = [\n",
    "        \"uuid\", \"date\",\n",
    "\n",
    "        *CORE_SENSORS, *PASSIVE_SENSORS, \"unlock_cnt\",\n",
    "        \"daily_event_cnt\", \"has_activity\",\n",
    "\n",
    "        # qc\n",
    "        \"qc_core_low_cov\",\"qc_core_very_low_activity\",\"qc_rhythm_low_cov\",\"qc_gap_low_cov\",\n",
    "        \"qc_meta_low_heartbeat\",\"qc_meta_retry_warn\",\"qc_meta_queue_warn\",\n",
    "\n",
    "        # signals (context)\n",
    "        \"partial_signal_raw\",\"travel_signal_raw\",\"tz_change_signal\",\n",
    "\n",
    "        # rhythm\n",
    "        \"rhythm_event_cnt\",\"rhythm_low_coverage\",\"night_ratio\",\"hour_entropy\",\n",
    "        \"day_ratio\",\"evening_ratio\",\"peak_hour\",\"peak_ratio\",\n",
    "\n",
    "        # gap\n",
    "        \"gap_event_cnt\",\"gap_low_coverage\",\"gap_max\",\"gap_p95\",\"gap_cnt_2h\",\"gap_cnt_6h\",\n",
    "        \"gap_long_ratio\",\"overnight_gap\",\"first_hour\",\"last_hour\",\n",
    "\n",
    "        # session\n",
    "        \"session_cnt\",\"session_total_sec\",\"session_mean_sec\",\"long_session_cnt\",\n",
    "\n",
    "        # mobility\n",
    "        \"cell_change_cnt\",\"wifi_change_cnt_est\", \"unique_wifi_cnt\", \"unique_cell_cnt\",\"step_sum\",\n",
    "\n",
    "        # meta\n",
    "        \"heartbeat_cnt\",\"retry_max\",\"queue_max\",\"tz_offset_min\",\"tz_offset_max\",\"tz_changed\",\n",
    "        \"qc_last_ts_max\",\n",
    "    ]\n",
    "\n",
    "    # delta 자동 추가\n",
    "    delta_cols = [c for c in df.columns if c.endswith(\"_d1\") or c.endswith(\"_r1\")]\n",
    "    cols = cols + delta_cols\n",
    "\n",
    "    # 실제 존재 컬럼만\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "\n",
    "    out = df[cols].copy()\n",
    "\n",
    "    # flag/signal류는 float로 통일 (0/1/NaN 유지)\n",
    "    for c in [\"partial_signal_raw\",\"travel_signal_raw\",\"tz_change_signal\"]:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    path = out_dir / \"daily_feature_v3.csv\"\n",
    "    out.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"✅ saved:\", path, \"| shape:\", out.shape)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb24c8",
   "metadata": {},
   "source": [
    "# 5. 저장 전 QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be706de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== QC REPORT: daily_feature_v3 =====\n",
      "shape: (9057, 48)\n",
      "users: 342 days: 90\n",
      "uuid-date dup: 0\n",
      "date null: 0\n",
      "\n",
      "[Missing Rate %] top:\n",
      "gap_cnt_6h_d1      9.29\n",
      "gap_cnt_2h_d1      9.29\n",
      "gap_p95_d1         9.29\n",
      "gap_max_d1         9.29\n",
      "night_ratio_d1     9.29\n",
      "hour_entropy_d1    9.29\n",
      "overnight_gap      5.21\n",
      "peak_hour          3.92\n",
      "hour_entropy       3.92\n",
      "day_ratio          3.92\n",
      "dtype: float64\n",
      "\n",
      "[Core Sum Check] mismatch rows: 0\n",
      "\n",
      "[Core Stats]\n",
      "                  count        mean         std  min    50%    90%      99%  \\\n",
      "Screen           9057.0  183.138788  132.310715  0.0  155.0  349.4   641.88   \n",
      "UserAct          9057.0  311.780170  256.841632  0.0  266.0  649.0  1138.88   \n",
      "daily_event_cnt  9057.0  494.918958  343.518544  0.0  439.0  947.4  1580.72   \n",
      "\n",
      "                    max  \n",
      "Screen           1280.0  \n",
      "UserAct          2073.0  \n",
      "daily_event_cnt  2501.0  \n",
      "daily_event_cnt==0 ratio: 0.13%\n",
      "\n",
      "[QC Flag Rates %]\n",
      "qc_core_low_cov: 3.92%\n",
      "qc_core_very_low_activity: 0.86%\n",
      "qc_rhythm_low_cov: 3.92%\n",
      "qc_gap_low_cov: 3.92%\n",
      "qc_meta_low_heartbeat: nan%\n",
      "qc_meta_retry_warn: nan%\n",
      "qc_meta_queue_warn: nan%\n",
      "\n",
      "[Range / Logic Violations] (count)\n",
      "No obvious violations found.\n",
      "\n",
      "[Delta Missing %] top:\n",
      "night_ratio_d1           9.29\n",
      "hour_entropy_d1          9.29\n",
      "gap_max_d1               9.29\n",
      "gap_cnt_2h_d1            9.29\n",
      "gap_p95_d1               9.29\n",
      "gap_cnt_6h_d1            9.29\n",
      "daily_event_cnt_d1       3.78\n",
      "travel_signal_raw_d1     3.78\n",
      "partial_signal_raw_d1    3.78\n",
      "tz_change_signal_d1      3.78\n",
      "dtype: float64\n",
      "\n",
      "[Suspect Samples] (first 10 rows)\n",
      "   uuid       date  daily_event_cnt  partial_signal_raw  travel_signal_raw  gap_max  gap_cnt_6h  night_ratio  hour_entropy\n",
      "u13cacd 2016-07-06                3                 1.0                0.0      NaN         NaN          NaN           NaN\n",
      "u1d4786 2016-06-16                7                 1.0                0.0      NaN         NaN          NaN           NaN\n",
      "u1e741e 2016-06-12                4                 1.0                0.0      NaN         NaN          NaN           NaN\n",
      "u1ef63c 2016-07-12                1                 1.0                0.0      NaN         NaN          NaN           NaN\n",
      "u1faf8a 2016-06-14                4                 1.0                0.0      NaN         NaN          NaN           NaN\n",
      "u31ae1c 2016-07-09                6                 1.0                0.0      NaN         NaN          NaN           NaN\n",
      "u327e1f 2016-07-06                7                 1.0                0.0      NaN         NaN          NaN           NaN\n",
      "u327e1f 2016-07-07                4                 1.0                0.0      NaN         NaN          NaN           NaN\n",
      "u337a42 2016-07-06                4                 1.0                0.0      NaN         NaN          NaN           NaN\n",
      "u337a42 2016-07-07                2                 1.0                0.0      NaN         NaN          NaN           NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DS3\\AppData\\Local\\Temp\\ipykernel_9376\\3691627718.py:11: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"rhythm_low_coverage\"] = df[\"rhythm_low_coverage\"].fillna(True).astype(bool)\n",
      "C:\\Users\\DS3\\AppData\\Local\\Temp\\ipykernel_9376\\3691627718.py:17: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"gap_low_coverage\"] = df[\"gap_low_coverage\"].fillna(True).astype(bool)\n"
     ]
    }
   ],
   "source": [
    "def feature_qc_report(df: pd.DataFrame, name=\"daily_feature_v3\", topk=10, exclude_meta_in_missing=True):\n",
    "    assert {\"uuid\",\"date\"}.issubset(df.columns), \"missing uuid/date\"\n",
    "    d = df.copy()\n",
    "    d[\"date\"] = pd.to_datetime(d[\"date\"], errors=\"coerce\").dt.normalize()\n",
    "\n",
    "    print(f\"\\n===== QC REPORT: {name} =====\")\n",
    "    print(\"shape:\", d.shape)\n",
    "    print(\"users:\", d[\"uuid\"].nunique(), \"days:\", d[\"date\"].nunique())\n",
    "    print(\"uuid-date dup:\", int(d.duplicated([\"uuid\",\"date\"]).sum()))\n",
    "    print(\"date null:\", int(d[\"date\"].isna().sum()))\n",
    "\n",
    "    # 1) 기본 결측률 (meta 제외 옵션)\n",
    "    META_COLS = [\n",
    "        \"heartbeat_cnt\",\"retry_max\",\"queue_max\",\"qc_last_ts_max\",\n",
    "        \"tz_offset_min\",\"tz_offset_max\",\"tz_changed\",\"tz_change_signal\",\n",
    "        \"qc_meta_low_heartbeat\",\"qc_meta_retry_warn\",\"qc_meta_queue_warn\",\n",
    "    ]\n",
    "\n",
    "    d_for_missing = d\n",
    "    if exclude_meta_in_missing:\n",
    "        drop_cols = [c for c in META_COLS if c in d_for_missing.columns]\n",
    "        d_for_missing = d_for_missing.drop(columns=drop_cols)\n",
    "\n",
    "    na = (d_for_missing.isna().mean().sort_values(ascending=False) * 100).round(2)\n",
    "    print(\"\\n[Missing Rate %] top:\")\n",
    "    print(na.head(topk))\n",
    "\n",
    "    core_cols = [c for c in CORE_SENSORS if c in d.columns]\n",
    "    if core_cols:\n",
    "        d[\"core_sum_check\"] = d[core_cols].sum(axis=1)\n",
    "        if \"daily_event_cnt\" in d.columns:\n",
    "            mismatch = (d[\"core_sum_check\"] != d[\"daily_event_cnt\"]).sum()\n",
    "            print(\"\\n[Core Sum Check] mismatch rows:\", int(mismatch))\n",
    "\n",
    "        print(\"\\n[Core Stats]\")\n",
    "        show_cols = core_cols + ([\"daily_event_cnt\"] if \"daily_event_cnt\" in d.columns else [])\n",
    "        print(d[show_cols].describe(percentiles=[.5,.9,.99]).T)\n",
    "\n",
    "        if \"daily_event_cnt\" in d.columns:\n",
    "            zero_days = (d[\"daily_event_cnt\"] == 0).mean() * 100\n",
    "            print(f\"daily_event_cnt==0 ratio: {zero_days:.2f}%\")\n",
    "\n",
    "    # 3) QC flag 비율\n",
    "    qc_cols = [c for c in d.columns if c.startswith(\"qc_\")]\n",
    "    if qc_cols:\n",
    "        print(\"\\n[QC Flag Rates %]\")\n",
    "        for c in qc_cols:\n",
    "            x = d[c]\n",
    "            if x.dtype == bool:\n",
    "                rate = x.mean() * 100\n",
    "            else:\n",
    "                rate = pd.to_numeric(x, errors=\"coerce\").mean() * 100\n",
    "            print(f\"{c}: {rate:.2f}%\")\n",
    "\n",
    "    # 4) 값 범위 검증(이상치/논리 위반)\n",
    "    checks = []\n",
    "\n",
    "    def add_check(col, cond, msg):\n",
    "        if col in d.columns:\n",
    "            n = int(cond(d[col]).sum())\n",
    "            checks.append((col, n, msg))\n",
    "\n",
    "    # ratio류 0~1 기대\n",
    "    add_check(\"night_ratio\", lambda s: (s<0) | (s>1), \"expected [0,1]\")\n",
    "    add_check(\"day_ratio\", lambda s: (s<0) | (s>1), \"expected [0,1]\")\n",
    "    add_check(\"evening_ratio\", lambda s: (s<0) | (s>1), \"expected [0,1]\")\n",
    "    add_check(\"peak_ratio\", lambda s: (s<0) | (s>1), \"expected [0,1]\")\n",
    "    add_check(\"gap_long_ratio\", lambda s: (s<0) | (s>1.5), \"expected ~[0,1] (allow 1.5 slack)\")\n",
    "\n",
    "    # 시간 관련\n",
    "    add_check(\"gap_max\", lambda s: s<0, \"gap should be >=0\")\n",
    "    add_check(\"gap_p95\", lambda s: s<0, \"gap should be >=0\")\n",
    "    add_check(\"overnight_gap\", lambda s: s<0, \"overnight_gap should be >=0\")\n",
    "    add_check(\"first_hour\", lambda s: (s<0) | (s>23), \"expected 0~23\")\n",
    "    add_check(\"last_hour\", lambda s: (s<0) | (s>23), \"expected 0~23\")\n",
    "    add_check(\"peak_hour\", lambda s: (s<0) | (s>23), \"expected 0~23\")\n",
    "\n",
    "    # 세션\n",
    "    add_check(\"session_total_sec\", lambda s: s<0, \"session_total_sec should be >=0\")\n",
    "    add_check(\"session_cnt\", lambda s: s<0, \"session_cnt should be >=0\")\n",
    "    add_check(\"session_mean_sec\", lambda s: s<0, \"session_mean_sec should be >=0\")\n",
    "\n",
    "    # meta\n",
    "    add_check(\"heartbeat_cnt\", lambda s: s<0, \"heartbeat_cnt should be >=0\")\n",
    "    add_check(\"retry_max\", lambda s: s<0, \"retry_max should be >=0\")\n",
    "    add_check(\"queue_max\", lambda s: s<0, \"queue_max should be >=0\")\n",
    "    add_check(\"tz_offset_min\", lambda s: (s<-720) | (s>840), \"tz offset minutes suspicious\")\n",
    "    add_check(\"tz_offset_max\", lambda s: (s<-720) | (s>840), \"tz offset minutes suspicious\")\n",
    "\n",
    "    if checks:\n",
    "        print(\"\\n[Range / Logic Violations] (count)\")\n",
    "        any_bad = False\n",
    "        for col, n, msg in checks:\n",
    "            if n > 0:\n",
    "                any_bad = True\n",
    "                print(f\"{col}: {n} rows -> {msg}\")\n",
    "        if not any_bad:\n",
    "            print(\"No obvious violations found.\")\n",
    "\n",
    "    # 5) delta sanity\n",
    "    delta_cols = [c for c in d.columns if c.endswith(\"_d1\") or c.endswith(\"_r1\")]\n",
    "    if delta_cols:\n",
    "        print(\"\\n[Delta Missing %] top:\")\n",
    "        delta_na = (d[delta_cols].isna().mean().sort_values(ascending=False) * 100).round(2)\n",
    "        print(delta_na.head(topk))\n",
    "\n",
    "    # 6) “의심 user-day” 샘플\n",
    "    suspect_cols = [c for c in [\n",
    "        \"daily_event_cnt\",\"heartbeat_cnt\",\"partial_signal_raw\",\"travel_signal_raw\",\n",
    "        \"gap_max\",\"gap_cnt_6h\",\"night_ratio\",\"hour_entropy\"\n",
    "    ] if c in d.columns]\n",
    "    if suspect_cols:\n",
    "        cond = pd.Series(False, index=d.index)\n",
    "\n",
    "        if \"daily_event_cnt\" in d.columns and \"heartbeat_cnt\" in d.columns:\n",
    "            cond |= (d[\"daily_event_cnt\"]==0) & (d[\"heartbeat_cnt\"].fillna(0)>0)\n",
    "        if \"partial_signal_raw\" in d.columns:\n",
    "            cond |= (d[\"partial_signal_raw\"].fillna(0)>0)\n",
    "\n",
    "        sus = d.loc[cond, [\"uuid\",\"date\"]+suspect_cols].head(10)\n",
    "        print(\"\\n[Suspect Samples] (first 10 rows)\")\n",
    "        print(sus.to_string(index=False))\n",
    "\n",
    "daily_feature_v3 = build_daily_feature_table_v3(CSV_PATHS)\n",
    "feature_qc_report(daily_feature_v3, name=\"daily_feature_v3\", exclude_meta_in_missing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e912f0",
   "metadata": {},
   "source": [
    "# 6. 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f710fd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ saved: ..\\artifacts\\features\\daily_feature_v3.csv | shape: (9057, 54)\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = Path(\"../artifacts/features\")\n",
    "_ = save_daily_feature_v3(daily_feature_v3, OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85891c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
